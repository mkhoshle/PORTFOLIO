{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext line_profiler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import scipy\n",
    "import scipy.stats as ss\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "from matplotlib import rc\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn.apionly as sns\n",
    "from IPython.display import HTML\n",
    "import os, glob, sys\n",
    "import warnings\n",
    "warnings.simplefilter('always')\n",
    "\n",
    "from bokeh.server.server import Server\n",
    "from bokeh.application import Application\n",
    "from bokeh.application.handlers.function import FunctionHandler\n",
    "from bokeh.plotting import figure, ColumnDataSource\n",
    "from bokeh.models.sources import ColumnDataSource\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show, push_notebook\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.plotting import figure \n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(all='raise', divide='ignore', over='raise', under='ignore', invalid='raise')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Generated Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('observation_single_particle.csv')\n",
    "data = data.drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(data['0'],data['1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N is Number of time levels and is obtained from the length of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wn = np.array(data['1'])\n",
    "N = len(Wn)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ffbs = FFBS()\n",
    "# A = np.zeros([4,N], dtype=np.float64)\n",
    "# # Likelihood or Emission distribution\n",
    "# ID = 0\n",
    "# p1 = hyper_param['gama']/(hyper_param['gama']+M-1)\n",
    "# pi0 = np.array([p1,0,0,0,1-p1,0,0,0]) \n",
    "# Snm = np.column_stack([[1,1,1,1,0,0,0,0],[0,1,2,3,0,1,2,3]]) #Snm[8,2]\n",
    "# F = ffbs.emission_distribution(Wn,Sn,params,M,ID,Snm)      # F[8,N]\n",
    "# pi_filter = update_pi_filter(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFBS:\n",
    "    def emission_distribution(self,x,Sn,params,M,ID,Snm):\n",
    "        # This is needed for the calculation of the Filter\n",
    "        ind = list(range(M))\n",
    "        ind.remove(ID)\n",
    "        self.Ksn = np.where(Sn==1,params['k_sigma2'],0.0) # Ksn[M,N]\n",
    "        self.Ksn_filter = [0.0,params['k_sigma2'],0.0,0.0,0.0,params['k_sigma2'],0.0,0.0] # Ksn_filter[8]\n",
    "        #self.rate [8,N]\n",
    "        r = params['theta']*hyper_param['dt']*(params['rho']+np.dot(params['bm'][0,ind],self.Ksn[ind,:]))\n",
    "        self.rate = [r+params['theta']*hyper_param['dt']*self.Ksn_filter[i] for i in range(2)]   \n",
    "        likelihood = [-self.rate[0]+x*np.log(self.rate[0]),-self.rate[1]+x*np.log(self.rate[1])]\n",
    "        \n",
    "        d = []\n",
    "        d.append(likelihood)\n",
    "        for i in range(6):\n",
    "            d.append(likelihood[0])         \n",
    "        \n",
    "        return np.vstack(d)\n",
    "    \n",
    "    def get_filter(self,A,pi0,pi_filter,N,F):\n",
    "        A[:,0] = np.log(pi0)\n",
    "        #=============================================================================================\n",
    "        for i in range(1,2):\n",
    "            A[0,i] = F[0,i]+A[0,i-1]+np.log(pi_filter[0,0])\n",
    "            #-------------------------------------------------------------------------------------\n",
    "            MM = np.array([(A[0,i-1]+np.log(pi_filter[0,1])),(A[1,i-1]+np.log(pi_filter[1,1])),(A[2,i-1]+np.log(pi_filter[2,1]))])\n",
    "            Max = -np.max(MM)\n",
    "            A[1,i] = F[1,i]-Max+np.log1p(np.sum(np.exp(np.delete(MM,np.argmax(MM))+Max)))\n",
    "            #--------------------------------------------------------------------------------------\n",
    "            MM = np.array([(A[1,i-1]+np.log(pi_filter[1,2])),(A[2,i-1]+np.log(pi_filter[2,2]))])\n",
    "            A[2,i] = F[2,i]+np.log(np.sum(np.exp(MM)))\n",
    "            #--------------------------------------------------------------------------------------\n",
    "            MM = np.array([(A[1,i-1]+np.log(pi_filter[1,3])),(A[3,i-1]+np.log(pi_filter[3,3]))])\n",
    "            A[3,i] = F[3,i]+np.log(np.sum(np.exp(MM)))\n",
    "            #--------------------------------------------------------------------------------------                \n",
    "            A[4,i] = F[4,i]+A[4,i-1]+np.log(pi_filter[4,4])\n",
    "            #--------------------------------------------------------------------------------------\n",
    "            MM = np.array([(A[4,i-1]+np.log(pi_filter[4,5])),(A[5,i-1]+np.log(pi_filter[5,5])),(A[6,i-1]+np.log(pi_filter[6,5]))])\n",
    "            A[5,i] = F[5,i]+np.log(np.sum(np.exp(MM)))\n",
    "            #--------------------------------------------------------------------------------------\n",
    "            MM = np.array([(A[5,i-1]+np.log(pi_filter[5,6])),(A[6,i-1]+np.log(pi_filter[6,6]))])\n",
    "            A[6,i] = F[6,i]+np.log(np.sum(np.exp(MM)))\n",
    "            #--------------------------------------------------------------------------------------\n",
    "            MM = np.array([(A[5,i-1]+np.log(pi_filter[5,7])),(A[7,i-1]+np.log(pi_filter[7,7]))])\n",
    "            A[7,i] = F[7,i]+np.log(np.sum(np.exp(MM)))\n",
    "        #=============================================================================================\n",
    "        for i in range(2,N):\n",
    "            A[0,i] = F[0,i]+A[0,i-1]+np.log(pi_filter[0,0])\n",
    "            #-------------------------------------------------------------------------------------\n",
    "            MM = np.array([(A[0,i-1]+np.log(pi_filter[0,1])),(A[1,i-1]+np.log(pi_filter[1,1])),(A[2,i-1]+np.log(pi_filter[2,1]))])\n",
    "            Max = -np.max(MM)\n",
    "            A[1,i] = F[1,i]-Max+np.log1p(np.sum(np.exp(np.delete(MM,np.argmax(MM))+Max)))\n",
    "            #--------------------------------------------------------------------------------------\n",
    "            MM = np.array([(A[1,i-1]+np.log(pi_filter[1,2])),(A[2,i-1]+np.log(pi_filter[2,2]))])\n",
    "            Max = -np.max(MM)\n",
    "            A[2,i] = F[2,i]-Max+np.log1p(np.sum(np.exp(np.delete(MM,np.argmax(MM))+Max)))\n",
    "            #--------------------------------------------------------------------------------------\n",
    "            MM = np.array([(A[1,i-1]+np.log(pi_filter[1,3])),(A[3,i-1]+np.log(pi_filter[3,3]))])\n",
    "            Max = -np.max(MM)\n",
    "            A[3,i] = F[3,i]-Max+np.log1p(np.sum(np.exp(np.delete(MM,np.argmax(MM))+Max)))\n",
    "            #--------------------------------------------------------------------------------------                \n",
    "            A[4,i] = F[4,i]+A[4,i-1]+np.log(pi_filter[4,4])\n",
    "            #--------------------------------------------------------------------------------------\n",
    "            MM = np.array([(A[4,i-1]+np.log(pi_filter[4,5])),(A[5,i-1]+np.log(pi_filter[5,5])),(A[6,i-1]+np.log(pi_filter[6,5]))])\n",
    "            Max = -np.max(MM)\n",
    "            A[5,i] = F[5,i]-Max+np.log1p(np.sum(np.exp(np.delete(MM,np.argmax(MM))+Max)))\n",
    "            #--------------------------------------------------------------------------------------\n",
    "            MM = np.array([(A[5,i-1]+np.log(pi_filter[5,6])),(A[6,i-1]+np.log(pi_filter[6,6]))])\n",
    "            Max = -np.max(MM)\n",
    "            A[6,i] = F[6,i]-Max+np.log1p(np.sum(np.exp(np.delete(MM,np.argmax(MM))+Max)))\n",
    "            #--------------------------------------------------------------------------------------\n",
    "            MM = np.array([(A[5,i-1]+np.log(pi_filter[5,7])),(A[7,i-1]+np.log(pi_filter[7,7]))])\n",
    "            Max = -np.max(MM)\n",
    "            A[7,i] = F[7,i]-Max+np.log1p(np.sum(np.exp(np.delete(MM,np.argmax(MM))+Max)))\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    # Filter changes for each particle because of the change in the rates. But transition matrix is the same.\n",
    "    def Filter(self,pi,pi_filter,N,ID,Sn,Snm,Wn,M,params,hyper_param):  \n",
    "        A = np.zeros([8,N], dtype=np.float64)\n",
    "        # Likelihood or Emission distribution\n",
    "        F = self.emission_distribution(Wn,Sn,params,M,ID,Snm)      # F[8,N]\n",
    "        \n",
    "        p1 = hyper_param['gama']/(hyper_param['gama']+M-1)\n",
    "        pi0 = np.array([p1,0,0,0,1-p1,0,0,0])   # pi0[8,1] \n",
    "#         A_star = pi0*F[:,0]\n",
    "#         c1 = np.sum(A_star)\n",
    "#         A[:,0] = A_star[:]/c1  # Array of 8*N where 8 is the number of possible states for the filter \n",
    "    \n",
    "        return self.get_filter(A,pi0,pi_filter,N,F)\n",
    "    \n",
    "    def _FFBS(self,N,pi,pi_filter,params):\n",
    "        # States of the system considered for the filter\n",
    "        Snm = np.column_stack([[1,1,1,1,0,0,0,0],[0,1,2,3,0,1,2,3]]) #Snm[8,2]\n",
    "        bm = np.zeros([1,M],dtype=int) \n",
    "        for ID in range(M):      \n",
    "            A = self.Filter(pi,pi_filter,N,ID,Sn,Snm,Wn,M,params,hyper_param) # A[8,N]\n",
    "            # Sanity Check\n",
    "            if np.sum(A,axis=0).all()==1:\n",
    "                pass\n",
    "            else:\n",
    "                raise Exception('Filter is not working for ID{} with {}'.format(ID,np.sum(A,axis=0)))        \n",
    "            \n",
    "            # Sampling via Gumbel distribution\n",
    "            g_i = -np.log(-np.log(np.random.uniform(size=8)))\n",
    "            ind_N = np.argmax(g_i+A[:,N-1])          # Get the state at the last time level\n",
    "            # Backward Sampling\n",
    "            Sn[ID,N-1] = Snm[ind_N,1]\n",
    "            bm[0,ID] = Snm[ind_N,0]\n",
    "            AA = A[:4,:] if ind_N<4 else A[4:,:]     # AA[4,N]           \n",
    "            for n in range(N-2,-1,-1):\n",
    "                weight = AA[:,n]+np.log(pi[:,Sn[ID,n+1]])\n",
    "                g_i = -np.log(-np.log(np.random.uniform(size=4)))\n",
    "                Sn[ID,n] = np.argmax(g_i+weight) \n",
    "          \n",
    "        return bm, Sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metropolis_Hastings:\n",
    "    def __init__(self,alfa1,alfa2):\n",
    "        self.alfa1 = alfa1\n",
    "        self.alfa2 = alfa2\n",
    "        \n",
    "    def _proposal(self,x,alfa):\n",
    "        return np.random.gamma(alfa,scale=x/alfa) \n",
    "    \n",
    "    def sample(self,rho_old,k_sigma2_old):\n",
    "        rho_prop = self._proposal(rho_old,self.alfa1) \n",
    "        k_sigma2_prop = self._proposal(k_sigma2_old,self.alfa2)\n",
    "        return rho_prop, k_sigma2_prop\n",
    "    \n",
    "    def _proposal_pdf(self,x1,x2,alfa):\n",
    "        return ((2*alfa-1)*(np.log(x1)-np.log(x2))+alfa*(x2/x1-x1/x2))\n",
    "    \n",
    "    def _prior(self,x1,x2,alfa,beta):\n",
    "        return (alfa-1)*(np.log(x2)-np.log(x1))+(x1-x2)/beta\n",
    "    \n",
    "    def _likelihood(self,x1,y1,x2,y2,Sn,params,hyper_param,Wn):\n",
    "        Ksn1 = np.where(Sn==1,y1,0.0)\n",
    "        rate1 = params['theta']*(x1+np.dot(params['bm'],Ksn1))*hyper_param['dt'] \n",
    "        \n",
    "        Ksn2 = np.where(Sn==1,y2,0.0)\n",
    "        rate2 = params['theta']*(x2+np.dot(params['bm'],Ksn2))*hyper_param['dt'] \n",
    "\n",
    "        return np.sum(Wn*(np.log(rate1)-np.log(rate2))+(rate2-rate1))\n",
    "    \n",
    "    def Acceptance_ratio(self,params,hyper_param,rho_prop,k_sigma2_prop,rho_old,k_sigma2_old,Wn):        \n",
    "        # Calculated the log of Prior_ratio, Proposal_ratio, and Likelihood_ratio\n",
    "        prior_ratio = (self._prior(rho_old,rho_prop,hyper_param['alfa1_prior'],hyper_param['beta1'])+\n",
    "                       self._prior(k_sigma2_old,k_sigma2_prop,hyper_param['alfa2_prior'],hyper_param['beta2']))         \n",
    "        \n",
    "        proposal_ratio = (self._proposal_pdf(rho_old,rho_prop,self.alfa1)+\n",
    "                          self._proposal_pdf(k_sigma2_old,k_sigma2_prop,self.alfa2))\n",
    "        \n",
    "        likelihood_ratio = self._likelihood(rho_prop,k_sigma2_prop,rho_old,k_sigma2_old,Sn,params,hyper_param,Wn) \n",
    "        \n",
    "        return likelihood_ratio+prior_ratio+proposal_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gibbs_sampler:  \n",
    "    def update_theta(self,alfa,beta,Wn,Ksn,params):\n",
    "        alfa_new = alfa+np.sum(Wn)\n",
    "        beta_new = beta+np.sum((params['rho']+np.dot(params['bm'][0,:],Ksn))*hyper_param['dt'])\n",
    "        \n",
    "        return np.random.gamma(alfa_new,1./beta_new)\n",
    "    \n",
    "    def update_roh_ksigma2(self,params,repeats,accepted):\n",
    "        MH = Metropolis_Hastings(hyper_param['alfa1_proposal'],hyper_param['alfa2_proposal'])\n",
    "        \n",
    "        for i in range(25):\n",
    "            rho_old = params['rho']\n",
    "            k_sigma2_old = params['k_sigma2']\n",
    "\n",
    "            # Step (a)\n",
    "            rho_prop,k_sigma2_prop = MH.sample(rho_old,k_sigma2_old)\n",
    "            # Step (b)\n",
    "            AR = MH.Acceptance_ratio(params,hyper_param,rho_prop,k_sigma2_prop,rho_old,k_sigma2_old,Wn)\n",
    "            # Step (c)\n",
    "            v = np.random.exponential(scale=1)\n",
    "            # Step (d)\n",
    "            if v>-AR:\n",
    "                accepted = accepted + 1.0\n",
    "                params['rho'] = rho_prop\n",
    "                params['k_sigma2'] = k_sigma2_prop\n",
    "            else:\n",
    "                params['rho'] = rho_old\n",
    "                params['k_sigma2'] = k_sigma2_old\n",
    "            \n",
    "        return params['rho'], params['k_sigma2'], accepted\n",
    "            \n",
    "    def update_pi(self,Sn,M):\n",
    "        # total number of transitions for all particles over time \n",
    "        # is M*(N-1)\n",
    "        count = dict({0:{0:0,1:0,2:0,3:0},1:{0:0,1:0,2:0,3:0},\n",
    "                      2:{0:0,1:0,2:0,3:0},3:{0:0,1:0,2:0,3:0}})\n",
    "        for j in range(M):\n",
    "            for i in range(N-1):\n",
    "                state1 = Sn[j,i]\n",
    "                if Sn[j,i+1]==0:\n",
    "                    count[state1][0] += 1\n",
    "                elif Sn[j,i+1]==1:\n",
    "                    count[state1][1] += 1\n",
    "                elif Sn[j,i+1]==2:\n",
    "                    count[state1][2] += 1\n",
    "                elif Sn[j,i+1]==3:\n",
    "                    count[state1][3] += 1\n",
    "                    \n",
    "        if count[0][1]>M:\n",
    "            raise Exception('Wrong transitions are happening')\n",
    "            \n",
    "        return count\n",
    "\n",
    "    def update_weight_State(self,hyper_param,pi,pi_filter,params):\n",
    "        ffbs = FFBS()\n",
    "        return ffbs._FFBS(N,pi,pi_filter,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pi_filter(pi):\n",
    "    aa = np.zeros([4,4],dtype=np.float64)\n",
    "    d1 = np.column_stack([pi,aa])\n",
    "    d2 = np.column_stack([aa,pi])\n",
    "    return np.vstack([d1,d2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __main__(repeats,M,pi,Sn,Ksn,handle):\n",
    "    data1 = 'Wn_single3.h5'\n",
    "    data2 = 'params_single3.h5'\n",
    "    data3 = 'trajectory_single3.h5'\n",
    "    \n",
    "    data = dict({'theta': [],'rho': [],'k_sigma2': [],'B':[], 'ite':[],'E_b':[],'E_ph':[]})\n",
    "\n",
    "    gibbs = Gibbs_sampler()\n",
    "    accepted  = 0.0\n",
    "    p = pd.DataFrame()\n",
    "    for j in range(repeats):\n",
    "        print(j)\n",
    "        start1 =  time.time()\n",
    "        params['theta'] = gibbs.update_theta(hyper_param['alfa'],hyper_param['beta'],Wn,Ksn,params)\n",
    "        # Metropolis Hastings\n",
    "        params['rho'], params['k_sigma2'], accepted = gibbs.update_roh_ksigma2(params,repeats,accepted)\n",
    "        # Update Transition Probabilities\n",
    "        count = gibbs.update_pi(Sn,M)\n",
    "\n",
    "        state = 0 \n",
    "        pi[0] = np.array(list(np.random.dirichlet((0.7+count[state][0],0.3+count[state][1])))+[0,0])\n",
    "        state = 1\n",
    "        pi[1] = np.array([0]+list(np.random.dirichlet((0.25+count[state][1],0.25+count[state][2],0.2+count[state][3]))))\n",
    "        state = 2 \n",
    "        pi[2] = np.array([0]+list(np.random.dirichlet((0.4+count[state][1],0.6+count[state][2])))+[0])\n",
    "        pi[3] = np.array([0,0,0,1])\n",
    "        \n",
    "        pi_filter = update_pi_filter(pi)\n",
    "        p = pd.concat([p,pd.DataFrame(pi)],axis=0)\n",
    "        \n",
    "        # Sanity Check\n",
    "        if all(np.sum(pi,axis=1))==1 and all(np.sum(pi_filter,axis=1))==1:\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception('probabilities do not sum to 1')      \n",
    "        print(params['theta']*params['rho'],params['theta']*params['k_sigma2'])\n",
    "        \n",
    "        # FFBS\n",
    "        params['bm'], Sn = gibbs.update_weight_State(hyper_param,pi,pi_filter,params)\n",
    "        \n",
    "        # Sanity Check\n",
    "        if all(Sn[:,0])==0:\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception('The initial state of the particle is not zero')\n",
    "            \n",
    "        # Store the Date\n",
    "        data['theta'].append(params['theta'])\n",
    "        data['rho'].append(params['rho'])\n",
    "        data['k_sigma2'].append(params['k_sigma2'])  \n",
    "        data['E_b'].append(params['theta']*params['rho'])\n",
    "        data['E_ph'].append(params['theta']*params['k_sigma2'])\n",
    "        data['B'].append(np.count_nonzero(params['bm']))      \n",
    "        data['ite'].append(j)\n",
    "    \n",
    "        # Timing\n",
    "        start2 = time.time()\n",
    "        print(start2-start1,np.count_nonzero(params['bm']),accepted)\n",
    "\n",
    "        # Save the output\n",
    "        ## MCMC Chain Parameters\n",
    "        df = pd.DataFrame(dict([(k,pd.Series(v)) for k,v in data.items()]))\n",
    "        params_data = pd.HDFStore(data2)\n",
    "        params_data['df{}'.format(j)] = df.loc[j]\n",
    "        params_data.close()\n",
    "        ## Emission Distribution\n",
    "        Ksn = np.where(Sn==1,params['k_sigma2'],0.0)\n",
    "        estimated_emission = params['theta']*(params['rho']+np.dot(params['bm'],Ksn))*hyper_param['dt']\n",
    "        estimated_emission_final = pd.DataFrame(dict({'ite':range(N),'est':estimated_emission.reshape(N,)}))\n",
    "        \n",
    "        if j%100==0:\n",
    "            emission_data = pd.HDFStore(data1)\n",
    "            emission_data['df{}'.format(j)] = pd.DataFrame(estimated_emission)\n",
    "            emission_data.close()\n",
    "            ## Trajectory of data\n",
    "            trajectory_data = pd.HDFStore(data3)\n",
    "            trajectory_data['df{}'.format(j)] = pd.DataFrame(Sn)\n",
    "            trajectory_data.close()\n",
    "            \n",
    "        # Some postprocessing\n",
    "#         new_data = dict({'E_b': [params['theta']*params['rho']],'E_ph': [params['theta']*params['k_sigma2']],\n",
    "#                          'B':[np.count_nonzero(params['bm'])],'ite':[j]})\n",
    "#         plot_traces(estimated_emission_final,new_data,handle)\n",
    "    \n",
    "    # Calculate Acceptance Rate\n",
    "    Acceptance_rate = accepted/repeats\n",
    "    print (\"Acceptance rate = \" + str(Acceptance_rate*100))\n",
    "    \n",
    "    return data, Acceptance_rate, p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameters:\n",
    "dt is the exposure time, alfa & beta are hyper-parameters of Theta, alfa1 & beta1 are hyper-parameters of Rho,\n",
    "alfa2 & beta2 are hyper-parameters of K_sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_param = {'alfa':1.0,'beta':1.0,'alfa1_proposal':10000,'beta1':1.0,'alfa1_prior':1.0,'alfa2_prior':1.0,\n",
    "               'alfa2_proposal':10000,'beta2':1.0,'N_particles':15,'gama':1.0,'dt':0.01*.95} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = hyper_param['N_particles']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters to be calculated by Gibbs Sampler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1 = 'Wn.h5'\n",
    "# data2 = 'params.h5'\n",
    "# params_data = pd.HDFStore(data2)\n",
    "# params = params_data['df{}'.format(15999)] \n",
    "# params = dict(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'theta':1e+1,\n",
    "          'rho':0.001,\n",
    "          'k_sigma2':10,\n",
    "          'bm':np.ones((1,M),dtype=int)}  # bm[1,M]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: \n",
    "The weights for the transition probability distribution are our hyper-parameters and we assume them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi0 = np.array(list(np.random.dirichlet((0.7,0.3)))+[0,0])\n",
    "pi1 = np.array([0]+list(np.random.dirichlet((0.55,0.25,0.2))))\n",
    "pi2 = np.array([0]+list(np.random.dirichlet((0.4,0.6)))+[0])\n",
    "pi3 = np.array([0,0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = np.stack((pi0,pi1,pi2,pi3),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pi = np.array([[9.95012479e-01, 4.72285706e-03, 2.40596046e-04, 2.40677016e-05],\n",
    "#        [0.00000000e+00, 8.95880600e-01, 9.46496180e-02, 9.46978182e-03],\n",
    "#        [0.00000000e+00, 9.46496180e-04, 9.99048684e-01, 4.82001787e-06],\n",
    "#        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: \n",
    "We know that the initial states of all particles is known and they are Inactive at the initial time level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data3 = 'trajectory_single_particle.h5'\n",
    "# trajectory_data = pd.HDFStore(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sn = np.zeros([M,N],dtype=np.int)\n",
    "Sn[:,0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Always double check to make sure wrong transitions are not happening, like more than one transition   \n",
    "# from state zero to state one and from state one to state zero.\n",
    "for i in range(M):\n",
    "    for j in range(1,N):\n",
    "        Sn[i,j] = np.random.choice(4,p=pi[Sn[i,j-1]])  #Sn[M,N] \n",
    "# Sn = pd.DataFrame(Sn)  # This is not efficient from computation point of view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sn = pd.read_csv('trajectory_single_particle.csv').drop('Unnamed: 0',axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ksn = np.where(Sn==1,params['k_sigma2'],0.0)   # Ksn[M,N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_traces(estimated_emission_final,new_data,handle):\n",
    "    source.stream(new_data)\n",
    "    source1.stream(estimated_emission_final)\n",
    "#     doc.add_periodic_callback(__main__)\n",
    "    push_notebook(handle=handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from line_profiler import LineProfiler\n",
    "if __name__ == '__main__':\n",
    "    if os.path.isfile('Wn_single3.h5'):\n",
    "        os.remove('Wn_single3.h5')\n",
    "        os.remove('params_single3.h5')\n",
    "        os.remove('trajectory_single3.h5')\n",
    "        \n",
    "    output_notebook()\n",
    "    source = ColumnDataSource({'E_b':[],'E_ph':[],'B':[],'ite':[]})\n",
    "    \n",
    "    s1 = figure(width=250, plot_height=250, title=None)\n",
    "    s1.scatter(x='ite', y='B', color=\"navy\", alpha=0.5, source=source)\n",
    "\n",
    "    s2 = figure(width=250, height=250, title=None)\n",
    "    s2.scatter(x='ite', y='E_b', color=\"firebrick\", alpha=0.5, source=source)\n",
    "\n",
    "    s3 = figure(width=350, height=250, title=None)\n",
    "    s3.scatter(x='ite', y='E_ph', color=\"olive\", alpha=0.5, source=source)\n",
    "    \n",
    "    source1 = ColumnDataSource(pd.DataFrame(columns=['ite','est']))\n",
    "    s4 = figure(width=350, height=250, title=None)\n",
    "    s4.scatter(x='ite', y='est', color=\"black\", alpha=0.5, source=source1)\n",
    "    s4.scatter(range(N), Wn, color=\"blue\", alpha=0.5)\n",
    "    \n",
    "    pl = gridplot([[s1,s2,s3],[s4]], toolbar_location=None)\n",
    "    handle = show(pl, notebook_handle=True, new='window', notebook_url='localhost:8888')\n",
    "    \n",
    "    out = __main__(repeats,M,pi,Sn,Ksn,handle) \n",
    "    # Use line-profiler for optimization\n",
    "#     lp = LineProfiler()\n",
    "#     lp.add_function(FFBS._FFBS)\n",
    "#     lp_wrapper = lp(__main__)\n",
    "#     lp_wrapper(repeats,M,pi,Sn,Ksn,pl)\n",
    "#     lp.print_stats(), format='pdf'\n",
    "#------------------------------------------------------\n",
    "#     %lprun -f __main__  __main__(repeats,M,pi,Sn,Ksn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = 'Wn_single3.h5'\n",
    "emission_data = pd.HDFStore(data1)\n",
    "fig = plt.figure(figsize=(4,4)) \n",
    "    \n",
    "plt.style.use('ggplot')\n",
    "sns.set_style('ticks')\n",
    "plt.rcParams['font.size'] = 12\n",
    "for i in range(100,1500,100):\n",
    "    plt.plot(range(N),emission_data['df{}'.format(i)].iloc[0]) \n",
    "plt.scatter(range(N), Wn, color=\"blue\", alpha=0.5)\n",
    "plt.xlim([0,20000])\n",
    "# plt.ylim([0,3000])\n",
    "# plt.ylabel(ylabel)\n",
    "# plt.xlabel(xlabel)\n",
    "plt.tight_layout()\n",
    "sns.despine(offset=10)\n",
    "# fig.savefig('Emission_distribution.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0.01\n",
    "partitions = [i*step for i in range(round(200./step))]\n",
    "t = partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Sn_true[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(t[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sn_true = pd.read_csv('trajectory_single_particle.csv').drop('Unnamed: 0',axis=1).values\n",
    "\n",
    "data3 = 'trajectory_single.h5'\n",
    "traj = pd.HDFStore(data3)\n",
    "fig = plt.figure(figsize=(4,4)) \n",
    "    \n",
    "plt.style.use('ggplot')\n",
    "sns.set_style('ticks')\n",
    "plt.rcParams['font.size'] = 12\n",
    "for i in range(100,5000,100):\n",
    "    plt.step(x=t[:-1],y=traj['df{}'.format(i)].iloc[0],color='red') \n",
    "plt.step(x=t[:-1],y=Sn_true[0,:], color=\"blue\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict([(k,pd.Series(v)) for k,v in out[0].items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = sns.jointplot(x=df['ite'],y=df['B'], kind='scatter',color='g',s =20,linewidth=2)\n",
    "h.ax_joint.set_ylim([0,25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns_plot = sns.jointplot(x=df['ite'],y=df['E_b'], kind='scatter',color='g',s =20,linewidth=2)\n",
    "sns.set(style=\"darkgrid\", color_codes=False)\n",
    "# sns_plot.savefig('Emission_background.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_plot = sns.jointplot(x=df['ite'],y=df['E_ph'], kind='scatter',color='g',s =20,linewidth=2)\n",
    "# sns_plot.ax_joint.set_ylim([0,5000])\n",
    "sns_plot.savefig('Photon_Emission.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = (scipy.special.kv(hyper_param['alfa']-hyper_param['alfa1_prior'],2*np.sqrt(np.linspace(1,40,1000))*(hyper_param['beta1']*hyper_param['beta2']))/\n",
    "     np.linspace(1,40,1000)**(1-(hyper_param['alfa']+hyper_param['alfa1_prior'])/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, bins, patches = plt.hist(out[0]['E_b'], 50, facecolor='blue')\n",
    "plt.plot(d*10000,color='k')\n",
    "plt.xlim([0,40])\n",
    "# plt.ylim([0,1])\n",
    "# plt.axvline(1000, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, bins, patches = plt.hist(out[0]['E_ph'], 50, facecolor='blue')\n",
    "plt.axvline(1e+3, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.step(x=t[:-1],y=Sn[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = out[2] \n",
    "pi_true = np.array([[9.95012479e-01, 4.72285706e-03, 2.40596046e-04, 2.40677016e-05],\n",
    "                    [0.00000000e+00, 8.95880600e-01, 9.46496180e-02, 9.46978182e-03],\n",
    "                    [0.00000000e+00, 9.46496180e-04, 9.99048684e-01, 4.82001787e-06],\n",
    "                    [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])\n",
    "d = dict([(index,0) for index,v in np.ndenumerate(pi)])\n",
    "for index, x in np.ndenumerate(pi):\n",
    "    i = index[0]\n",
    "    j = index[1]\n",
    "    d[index] = list(out1.iloc[i::4,j])\n",
    "    \n",
    "def plot_hist(dic, pi_true):\n",
    "    fig = plt.figure(figsize=(16,16)) \n",
    "    plt.style.use('ggplot')\n",
    "    sns.set_style('ticks')\n",
    "    plt.rcParams['font.size'] = 12\n",
    "\n",
    "    nn = 1\n",
    "    for i, x in np.ndenumerate(pi_true):\n",
    "        ax = fig.add_subplot(4,4,nn)\n",
    "        n, bins, patches = plt.hist(dic[i], 10, facecolor='blue')\n",
    "        plt.axvline(pi_true[i], color='r')\n",
    "#         ax.set_title(r'$particle-ID: {}$'.format(str(i)))\n",
    "#         ax.legend().set_visible(False)\n",
    "        sns.despine(offset=12,ax=ax,trim=True)\n",
    "        nn = nn+1\n",
    "\n",
    "    # plt.subplots_adjust(top=0.92,bottom=0.08,left=0.1,right=0.95,wspace=0.6,hspace=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show() \n",
    "    fig.savefig('Probability_distribution.pdf')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(d, pi_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
