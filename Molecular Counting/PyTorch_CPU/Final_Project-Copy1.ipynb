{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahzadkhoshlessan/miniconda3/envs/ml/lib/python3.5/site-packages/matplotlib/__init__.py:855: MatplotlibDeprecationWarning: \n",
      "examples.directory is deprecated; in the future, examples will be found relative to the 'datapath' directory.\n",
      "  \"found relative to the 'datapath' directory.\".format(key))\n",
      "/Users/mahzadkhoshlessan/miniconda3/envs/ml/lib/python3.5/site-packages/matplotlib/__init__.py:846: MatplotlibDeprecationWarning: \n",
      "The text.latex.unicode rcparam was deprecated in Matplotlib 2.2 and will be removed in 3.1.\n",
      "  \"2.2\", name=key, obj_type=\"rcparam\", addendum=addendum)\n",
      "/Users/mahzadkhoshlessan/miniconda3/envs/ml/lib/python3.5/site-packages/seaborn/apionly.py:9: UserWarning: As seaborn no longer sets a default style on import, the seaborn.apionly module is deprecated. It will be removed in a future version.\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial import distance\n",
    "import re, sys, os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "from IPython.display import display\n",
    "import seaborn.apionly as sns\n",
    "plt.style.use('ggplot')\n",
    "sns.set_style('ticks')\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "from torchvision import datasets\n",
    "from IPython.display import Image\n",
    "import h5py\n",
    "import numpy\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data_utils\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Enable inline plotting  \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Applies a 2D convolution over an input signal composed of several input\n",
       "planes.\n",
       "\n",
       "In the simplest case, the output value of the layer with input size\n",
       ":math:`(N, C_{\\text{in}}, H, W)` and output :math:`(N, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})`\n",
       "can be precisely described as:\n",
       "\n",
       ".. math::\n",
       "    \\text{out}(N_i, C_{\\text{out}_j}) = \\text{bias}(C_{\\text{out}_j}) +\n",
       "    \\sum_{k = 0}^{C_{\\text{in}} - 1} \\text{weight}(C_{\\text{out}_j}, k) \\star \\text{input}(N_i, k)\n",
       "\n",
       "\n",
       "where :math:`\\star` is the valid 2D `cross-correlation`_ operator,\n",
       ":math:`N` is a batch size, :math:`C` denotes a number of channels,\n",
       ":math:`H` is a height of input planes in pixels, and :math:`W` is\n",
       "width in pixels.\n",
       "\n",
       "* :attr:`stride` controls the stride for the cross-correlation, a single\n",
       "  number or a tuple.\n",
       "\n",
       "* :attr:`padding` controls the amount of implicit zero-paddings on both\n",
       "  sides for :attr:`padding` number of points for each dimension.\n",
       "\n",
       "* :attr:`dilation` controls the spacing between the kernel points; also\n",
       "  known as the Ã  trous algorithm. It is harder to describe, but this `link`_\n",
       "  has a nice visualization of what :attr:`dilation` does.\n",
       "\n",
       "* :attr:`groups` controls the connections between inputs and outputs.\n",
       "  :attr:`in_channels` and :attr:`out_channels` must both be divisible by\n",
       "  :attr:`groups`. For example,\n",
       "\n",
       "    * At groups=1, all inputs are convolved to all outputs.\n",
       "    * At groups=2, the operation becomes equivalent to having two conv\n",
       "      layers side by side, each seeing half the input channels,\n",
       "      and producing half the output channels, and both subsequently\n",
       "      concatenated.\n",
       "    * At groups= :attr:`in_channels`, each input channel is convolved with\n",
       "      its own set of filters, of size:\n",
       "      :math:`\\left\\lfloor\\frac{C_\\text{out}}{C_\\text{in}}\\right\\rfloor`.\n",
       "\n",
       "The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:\n",
       "\n",
       "    - a single ``int`` -- in which case the same value is used for the height and width dimension\n",
       "    - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,\n",
       "      and the second `int` for the width dimension\n",
       "\n",
       ".. note::\n",
       "\n",
       "     Depending of the size of your kernel, several (of the last)\n",
       "     columns of the input might be lost, because it is a valid `cross-correlation`_,\n",
       "     and not a full `cross-correlation`_.\n",
       "     It is up to the user to add proper padding.\n",
       "\n",
       ".. note::\n",
       "\n",
       "    When `groups == in_channels` and `out_channels == K * in_channels`,\n",
       "    where `K` is a positive integer, this operation is also termed in\n",
       "    literature as depthwise convolution.\n",
       "\n",
       "    In other words, for an input of size :math:`(N, C_{in}, H_{in}, W_{in})`,\n",
       "    a depthwise convolution with a depthwise multiplier `K`, can be constructed by arguments\n",
       "    :math:`(in\\_channels=C_{in}, out\\_channels=C_{in} \\times K, ..., groups=C_{in})`.\n",
       "\n",
       ".. include:: cudnn_deterministic.rst\n",
       "\n",
       "Args:\n",
       "    in_channels (int): Number of channels in the input image\n",
       "    out_channels (int): Number of channels produced by the convolution\n",
       "    kernel_size (int or tuple): Size of the convolving kernel\n",
       "    stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
       "    padding (int or tuple, optional): Zero-padding added to both sides of the input. Default: 0\n",
       "    dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n",
       "    groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1\n",
       "    bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``\n",
       "\n",
       "Shape:\n",
       "    - Input: :math:`(N, C_{in}, H_{in}, W_{in})`\n",
       "    - Output: :math:`(N, C_{out}, H_{out}, W_{out})` where\n",
       "\n",
       "      .. math::\n",
       "          H_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] - \\text{dilation}[0]\n",
       "                    \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\n",
       "\n",
       "      .. math::\n",
       "          W_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] - \\text{dilation}[1]\n",
       "                    \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\n",
       "\n",
       "Attributes:\n",
       "    weight (Tensor): the learnable weights of the module of shape\n",
       "                     (out_channels, in_channels, kernel_size[0], kernel_size[1]).\n",
       "                     The values of these weights are sampled from\n",
       "                     :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
       "                     :math:`k = \\frac{1}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}`\n",
       "    bias (Tensor):   the learnable bias of the module of shape (out_channels). If :attr:`bias` is ``True``,\n",
       "                     then the values of these weights are\n",
       "                     sampled from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
       "                     :math:`k = \\frac{1}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}`\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> # With square kernels and equal stride\n",
       "    >>> m = nn.Conv2d(16, 33, 3, stride=2)\n",
       "    >>> # non-square kernels and unequal stride and with padding\n",
       "    >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n",
       "    >>> # non-square kernels and unequal stride and with padding and dilation\n",
       "    >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n",
       "    >>> input = torch.randn(20, 16, 50, 100)\n",
       "    >>> output = m(input)\n",
       "\n",
       ".. _cross-correlation:\n",
       "    https://en.wikipedia.org/wiki/Cross-correlation\n",
       "\n",
       ".. _link:\n",
       "    https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
       "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/ml/lib/python3.5/site-packages/torch/nn/modules/conv.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.Conv2d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/Users/mahzadkhoshlessan/Desktop/Machine-Learning/Intro-to-Deep-Learning/Final_Project/PyTorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CNN_Model import project_01, normalize_im, NeuralNet, L1L2loss, weight_init, Checkpoint\n",
    "from DataLoader import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training & test data as training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5Train = h5py.File('../Input/TrainingSplit.h5', 'r')\n",
    "h5Test = h5py.File('../Input/TestSplit.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = {'train': list(h5Train.keys()), 'test': list(h5Test.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = dict()\n",
    "P['numEpoch'] = 6\n",
    "P['learning_rate'] = 0.001\n",
    "P['batchSize'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generators\n",
    "training_set = Dataset(h5Train,P['batchSize'],partition['train'])\n",
    "train_loader = DataLoader(training_set, shuffle=True)\n",
    "\n",
    "validation_set = Dataset(h5Test,P['batchSize'],partition['test'])\n",
    "test_loader = DataLoader(validation_set, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel = NeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psize = 208\n",
    "optimizer = optim.Adam(myModel.parameters(), lr=P['learning_rate'])\n",
    "criterion = L1L2loss((1, psize, psize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel.apply(weight_init)\n",
    "scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=5, min_lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss history recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=('epoch', 'loss', 'loss_test'))\n",
    "Weight_file = os.path.abspath(os.path.normpath(os.path.join(os.getcwd(),'Checkpointing/Checkpointer.h5')))\n",
    "meanstd_name = os.path.abspath(os.path.normpath(os.path.join(os.getcwd(),'meanstd_name.mat')))\n",
    "path = os.path.abspath(os.path.normpath(os.path.join(os.getcwd(),'output.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 7000\n",
    "N_test = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = {'train':[],'test':[]}\n",
    "loss_c = {'train':np.inf,'test':np.inf}\n",
    "data_loader = {'train':train_loader,'test':test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = {'train':[],'test':[]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list['test'].append(4.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': [2.5, 3.5, 6.5, 5.5, 4.5], 'train': []}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_best = bool(1.5 < min(loss_list['test']))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(P['numEpoch']):\n",
    "    for phase in ['train', 'test']:\n",
    "        if phase == 'train':\n",
    "            myModel.train()\n",
    "        else:\n",
    "            myModel.eval()\n",
    "    \n",
    "        running_loss = 0.0\n",
    "        counter = 0\n",
    "        print(phase)\n",
    "        for i, (images, labels) in enumerate(data_loader[phase]):\n",
    "            # Run the forward pass\n",
    "            counter = counter+1\n",
    "            images = images.reshape(P['batchSize'], 1, psize, psize)\n",
    "            labels = labels.reshape(P['batchSize'], 1, psize, psize) \n",
    "            \n",
    "            density_pred = myModel(images)\n",
    "            loss = criterion(labels,density_pred)\n",
    "        \n",
    "            if phase == 'train':\n",
    "                scheduler.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                scheduler.step(loss_c['test']) \n",
    "\n",
    "            running_loss += loss.detach().numpy()\n",
    "            \n",
    "            if phase=='train' and counter==400:\n",
    "                break\n",
    "            if phase=='test' and counter==187:\n",
    "                break\n",
    "                \n",
    "        running_loss *= P['batchSize']/counter\n",
    "        loss_list[phase].append(running_loss) \n",
    "\n",
    "        # Save the model weights after each epoch if the validation loss decreased\n",
    "        loss_c[phase] = running_loss\n",
    "        if phase=='test':\n",
    "            if loss_list[phase]:\n",
    "                is_best = bool(loss_c[phase] < min(loss_list[phase]))    \n",
    "                Checkpoint(myModel, optimizer, epoch, is_best, Weight_file) \n",
    "    \n",
    "        loss_list[phase].append(running_loss) \n",
    "\n",
    "    # B.3) stat for this epoch\n",
    "    print(str(epoch)+'    loss:'+str(loss_c['train'])+' '+str(loss_c['test']))\n",
    "    # save\n",
    "    df.loc[epoch] = [epoch, loss_c['train'], loss_c['test']]\n",
    "    df.to_csv(path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdict = {\"mean_test\": mean_val_test, \"std_test\": std_val_test}                  \n",
    "sio.savemat(meanstd_name, mdict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = pd.read_csv('keras-data/Keras_loss.csv', sep=r'\\t',engine='python', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff1 = pd.read_csv('keras-data/Keras-loss-no-scheduler.csv', sep=r'\\t',engine='python', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff2 = pd.read_csv('keras-data/output111.csv', sep=r',',engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff3 = pd.read_csv('keras-data/output.csv', sep=r',',engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "ax.plot(dff[0],dff[1],'-o')\n",
    "ax.plot(dff[0],dff[2],'-o')\n",
    "\n",
    "ax.plot(dff1[0],dff1[1],'-o')\n",
    "ax.plot(dff1[0],dff1[2],'-o')\n",
    "\n",
    "ax.plot(dff2['epoch'],dff2['loss'],'-o')\n",
    "ax.plot(dff2['epoch'],dff2['loss_test'],'-o')\n",
    "\n",
    "# ax.plot(dff3['epoch'],dff3['loss'],'-o')\n",
    "# ax.plot(dff3['epoch'],dff3['loss_test'],'-o')\n",
    "\n",
    "ax.grid()\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "\n",
    "ax.legend(['train loss Keras', 'test loss Keras','train loss Keras-No Scheduler', 'test loss Keras-No Scheduler',\n",
    "           'train loss Torch', 'test loss Torch'])\n",
    "\n",
    "fig.savefig('loss_out.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.plot(dff3['epoch'],dff3['loss'],'-o')\n",
    "ax.plot(dff3['epoch'],dff3['loss_test'],'-o')\n",
    "\n",
    "ax.grid()\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "\n",
    "ax.legend(['train loss Torch-No Scheduler-No Init', 'test loss Torch-No Scheduler-No Init'])\n",
    "fig.savefig('loss_out2.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    checkpoint = torch.load(resume_weights)\n",
    "else:\n",
    "    # Load GPU model on CPU\n",
    "    checkpoint = torch.load(resume_weights,\n",
    "                            map_location=lambda storage,\n",
    "                            loc: storage)\n",
    "start_epoch = checkpoint['epoch']\n",
    "best_accuracy = checkpoint['best_accuracy']\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "print(\"=> loaded checkpoint '{}' (trained for {} epochs)\".format(resume_weights, checkpoint['epoch']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
