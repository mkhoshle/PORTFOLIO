{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial import distance\n",
    "import re, sys, os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "from IPython.display import display\n",
    "import seaborn.apionly as sns\n",
    "plt.style.use('ggplot')\n",
    "sns.set_style('ticks')\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "from torchvision import datasets\n",
    "from IPython.display import Image\n",
    "import h5py\n",
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano import pp\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data_utils\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# sys.path.insert(0, \"/Users/mahzadkhoshlessan/Desktop/Machine-Learning/Intro-to-Deep-Learning/Final_Project/\")\n",
    "# from CNN_Model import project_01\n",
    "\n",
    "# Enable inline plotting  \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/Users/mahzadkhoshlessan/Desktop/Machine-Learning/Intro-to-Deep-Learning/Final_Project/PyTorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CNN_Model import project_01, normalize_im, NeuralNet, L1L2loss, weight_init, Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data and divide it to training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "matfile = h5py.File('../Input/TrainingSet.mat', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['heatmaps', 'patches', 'spikes']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(matfile.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = np.array(matfile['patches'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patches = patches[:50,:,:]    #**********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 208, 208)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmaps = 100.0*np.array(matfile['heatmaps'])   # Why multiply by 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmaps = heatmaps[:50,:,:]  #**********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(patches, heatmaps, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting type\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 208, 208)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training Examples: 7000\n",
      "Number of Validation Examples: 3000\n"
     ]
    }
   ],
   "source": [
    "print('Number of Training Examples: %d' % X_train.shape[0])\n",
    "print('Number of Validation Examples: %d' % X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================== Training set normalization ==========================\n",
    "# normalize training images to be in the range [0,1] and calculate the \n",
    "# training set mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train = np.zeros(X_train.shape[0],dtype=np.float32)\n",
    "std_train = np.zeros(X_train.shape[0], dtype=np.float32)\n",
    "for i in range(X_train.shape[0]):\n",
    "    X_train[i, :, :] = project_01(X_train[i, :, :])\n",
    "    mean_train[i] = X_train[i, :, :].mean()\n",
    "    std_train[i] = X_train[i, :, :].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resulting normalized training images\n",
    "mean_val_train = mean_train.mean()\n",
    "std_val_train = std_train.mean()\n",
    "X_train_norm = np.zeros(X_train.shape, dtype=np.float32)\n",
    "for i in range(X_train.shape[0]):\n",
    "    X_train_norm[i, :, :] = normalize_im(X_train[i, :, :], mean_val_train, std_val_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch size (Input Dim)\n",
    "psize = X_train_norm.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Test set normalization ==========================\n",
    "# normalize test images to be in the range [0,1] and calculate the test set \n",
    "# mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_test = np.zeros(X_test.shape[0],dtype=np.float32)\n",
    "std_test = np.zeros(X_test.shape[0], dtype=np.float32)\n",
    "for i in range(X_test.shape[0]):\n",
    "    X_test[i, :, :] = project_01(X_test[i, :, :])\n",
    "    mean_test[i] = X_test[i, :, :].mean()\n",
    "    std_test[i] = X_test[i, :, :].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resulting normalized test images\n",
    "mean_val_test = mean_test.mean()\n",
    "std_val_test = std_test.mean()\n",
    "X_test_norm = np.zeros(X_test.shape, dtype=np.float32)\n",
    "for i in range(X_test.shape[0]):\n",
    "    X_test_norm[i, :, :] = normalize_im(X_test[i, :, :], mean_val_test, std_val_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = dict()\n",
    "P['numEpoch'] = 6\n",
    "P['learning_rate'] = 0.001\n",
    "P['batchSize'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping: nn.Conv2d expects an input of the shape [batch_size, channels, height, width]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = X_train_norm.view(X_train.shape[0], 1, psize, psize)\n",
    "X_test_norm = X_test_norm.view(X_test.shape[0], 1, psize, psize)\n",
    "y_train = y_train.view(y_train.shape[0], 1, psize, psize)\n",
    "y_test = y_test.view(y_test.shape[0], 1, psize, psize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_utils.TensorDataset(X_train_norm, y_train)\n",
    "test = data_utils.TensorDataset(X_test_norm, y_test)\n",
    "\n",
    "train_loader = DataLoader(train,shuffle=True,batch_size=P['batchSize'])\n",
    "test_loader = DataLoader(test,shuffle=False,batch_size=P['batchSize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel = NeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(myModel.parameters(), lr=P['learning_rate'])\n",
    "criterion = L1L2loss((1, psize, psize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel.apply(weight_init)\n",
    "scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=5, min_lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss history recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=('epoch', 'loss', 'loss_test'))\n",
    "Weight_file = os.path.abspath(os.path.normpath(os.path.join(os.getcwd(),'Checkpointing/Checkpointer.h5')))\n",
    "meanstd_name = os.path.abspath(os.path.normpath(os.path.join(os.getcwd(),'meanstd_name.mat')))\n",
    "path = os.path.abspath(os.path.normpath(os.path.join(os.getcwd(),'output.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_dataset = {'train':N_train,'test':N_test}\n",
    "loss_list = {'train':[],'test':[]}\n",
    "loss_c = {'train':np.inf,'test':np.inf}\n",
    "data_loader = {'train':train_loader,'test':test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(P['numEpoch']):\n",
    "    for phase in ['train', 'test']:\n",
    "        if phase == 'train':\n",
    "            myModel.train()\n",
    "        else:\n",
    "            myModel.eval()\n",
    "    \n",
    "        running_loss = 0.0\n",
    "        counter = 0\n",
    "        for i, (images, labels) in enumerate(data_loader[phase]):\n",
    "            # Run the forward pass\n",
    "            counter = counter+1\n",
    "            if phase=='train' and counter==400:\n",
    "                break\n",
    "\n",
    "            density_pred = myModel(images)\n",
    "            loss = criterion(labels,density_pred)\n",
    "        \n",
    "            if phase == 'train':\n",
    "                scheduler.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                scheduler.step(loss_c['test']) \n",
    "\n",
    "            running_loss += loss.detach().numpy()\n",
    "\n",
    "        running_loss *= P['batchSize']/N_train \n",
    "        loss_list[phase].append(running_loss) \n",
    "\n",
    "        # Save the model weights after each epoch if the validation loss decreased\n",
    "        loss_c[phase] = running_loss\n",
    "        if phase=='test':\n",
    "            if loss_list['test']:\n",
    "                is_best = bool(loss_c['test'] < min(loss_list['test']))    \n",
    "                Checkpoint(myModel, optimizer, epoch, is_best, Weight_file) \n",
    "    \n",
    "        loss_list[phase].append(running_loss) \n",
    "\n",
    "    # B.3) stat for this epoch\n",
    "    print(str(epoch)+'    loss:'+str(loss_c['train'])+' '+str(loss_c['test']))\n",
    "    # save\n",
    "    df.loc[epoch] = [epoch, loss_c['train'], loss_c['test']]\n",
    "    df.to_csv(path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdict = {\"mean_test\": mean_val_test, \"std_test\": std_val_test}                  \n",
    "sio.savemat(meanstd_name, mdict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = pd.read_csv('keras-data/Keras_loss.csv', sep=r'\\t',engine='python', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff1 = pd.read_csv('keras-data/Keras-loss-no-scheduler.csv', sep=r'\\t',engine='python', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff2 = pd.read_csv('keras-data/output111.csv', sep=r',',engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff3 = pd.read_csv('keras-data/output.csv', sep=r',',engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "ax.plot(dff[0],dff[1],'-o')\n",
    "ax.plot(dff[0],dff[2],'-o')\n",
    "\n",
    "ax.plot(dff1[0],dff1[1],'-o')\n",
    "ax.plot(dff1[0],dff1[2],'-o')\n",
    "\n",
    "ax.plot(dff2['epoch'],dff2['loss'],'-o')\n",
    "ax.plot(dff2['epoch'],dff2['loss_test'],'-o')\n",
    "\n",
    "# ax.plot(dff3['epoch'],dff3['loss'],'-o')\n",
    "# ax.plot(dff3['epoch'],dff3['loss_test'],'-o')\n",
    "\n",
    "ax.grid()\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "\n",
    "ax.legend(['train loss Keras', 'test loss Keras','train loss Keras-No Scheduler', 'test loss Keras-No Scheduler',\n",
    "           'train loss Torch', 'test loss Torch'])\n",
    "\n",
    "fig.savefig('loss_out.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.plot(dff3['epoch'],dff3['loss'],'-o')\n",
    "ax.plot(dff3['epoch'],dff3['loss_test'],'-o')\n",
    "\n",
    "ax.grid()\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "\n",
    "ax.legend(['train loss Torch-No Scheduler-No Init', 'test loss Torch-No Scheduler-No Init'])\n",
    "fig.savefig('loss_out2.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    checkpoint = torch.load(resume_weights)\n",
    "else:\n",
    "    # Load GPU model on CPU\n",
    "    checkpoint = torch.load(resume_weights,\n",
    "                            map_location=lambda storage,\n",
    "                            loc: storage)\n",
    "start_epoch = checkpoint['epoch']\n",
    "best_accuracy = checkpoint['best_accuracy']\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "print(\"=> loaded checkpoint '{}' (trained for {} epochs)\".format(resume_weights, checkpoint['epoch']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
